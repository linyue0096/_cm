# 觀念題型
## 1.線性代數中的『線性』指的是什麼？為何要稱為『代數』
* 什麼是[線性]？
(1) 必須是平直的(限制範圍)
(2) 幾何上(二、三維空間)：平直的空間（直線、平面、超平面）。
(3) 代數上：疊加原理
* 加法性 $f(x+y) = f(x) + f(y)$
* 齊次性/比例性 $f(ax) = a \cdot f(x)$ 

* 為何稱為[代數]？
(1) 抽象符號運算(提供工具)
(2) 將具體數字抽象為向量與矩陣(非線性)
(3) 透過系統性的運算規則（如 $Ax=b$）解決龐大的結構性問題
--------------------------------------------------
## 2.數學中的『空間』是什麼？為何『向量空間』被稱為空間
* 數學中的[空間]是什麼？
在數學中，指一個集合 (Set) + 一套規則，只要符合這套規則，就屬於這個空間。
* 為何稱為「向量空間」？
它提供了封閉且完備的活動範圍，所以必須滿足兩大封閉性：
(1) 加法封閉：空間內任意兩個向量相加，結果仍在空間內。
(2) 數乘封閉：任意向量被拉長或縮短，結果仍在空間內。

意義：保證了我們可以無限進行線性運算，不管怎麼移動都在範圍中，而永遠不會[跑出]這個系統
--------------------------------------------------
## 3.矩陣和向量之間有何關係？矩陣代表的意義是什麼？
* 矩陣和向量之間有何關係？
在數學中，指函數 (Function)與輸入(Input)的關係。若將向量視為空間中的一個靜態的點(Point)或數據，矩陣就是負責移動這個點、處理這筆數據的動作或機器。
* 矩陣代表的意義是什麼？
它描述了一種線性變換的規則，將空間進行扭曲或映射，其核心價值在於兩大轉換能力
(1) 幾何變換(運動):矩陣作用於向量（$Ax = b$），等於對該向量發號施令，使其發生 旋轉、伸縮、剪切或投影
(2) 空間映射:矩陣可以看作是連接兩個不同維度或不同座標系的橋樑，將向量從一個空間「映射」到另一個空間。

意義:保證了我們可以把複雜的幾何運動簡化為代數上的 [乘法運算]。不管空間如何被扭曲，原點固定且網格線保持平行，讓我們能掌握整個空間是如何被 [操縱] 與 [變形] 的。
-----------------------------------------------
## 4.如何用矩陣代表2D/3D幾何學中的『平移，縮放，旋轉』操作？
* 什麼是幾何操作的矩陣表示？
在數學與電腦圖學中，指 仿射變換與齊次座標的應用。為了統一處理所有操作，我們必須將 $N$ 維空間的點提升到 $N+1$ 維
* 如何同時達成「平移、縮放、旋轉？
單純的線性變換矩陣無法處理「平移」（原點不能移動），它透過擴充矩陣維度來提供額外的操作空間，構造出一個功能完備的「變換矩陣」
(1) 縮放與旋轉(線性部分):由矩陣左上角的 $N \times N$ 區塊控制。它負責對空間進行拉伸或轉動（改變基底向量的方向與長度）。
(2) 平移(位移部分):由矩陣 最右側 的一欄控制。它利用齊次座標中多出來的維度，將[加法位移]偽裝成[矩陣乘法]。

意義:保證了所有幾何操作都可以統一簡化為[矩陣連乘]。不管經歷多少次複雜的先旋轉、再縮放、最後平移，我們都可以將這一長串步驟壓縮成[單一個矩陣]，極大化了運算效率（這也是GPU運作的核心原理）。
-----------------------------------------------------
## 5.行列式的意義是什麼？如何用遞迴公式計算矩陣的行列式？行列式和體積有什麼關係？
* 數學中的[行列式]是什麼？
在數學中，指一個純量數值。用單一個數字來壓縮並概括矩陣對空間造成的幾何改變程度。
* 行列式和體積有什麼關係？
它提供了衡量空間變換後[伸縮率]的標準
(1) 體積縮放率:行列式的絕對值 $|det(A)|$ 代表變換後的幾何圖形體積是原本的幾倍。若數值為 $0$，代表體積被壓扁至零（降維），矩陣不可逆。
(2) 方向性:數值的正負號代表空間是否被 翻轉。若為負值，表示該變換像鏡子一樣改變了座標系的左右手定則方向。

* 如何計算行列式？
(慢到快速)計算行列式的方法代表了從「理論定義」到「工程實用」的跨越
(1) 遞迴公式:將 $n \times n$ 矩陣降階拆解。
方法:選擇一行或一列，將元素乘以其對應的[餘因子]後加總。
(2) 透過對角化快速計算:
原理：利用特徵值。若矩陣 $A$ 可以對角化為 $PDP^{-1}$，則 $det(A) = det(D)$。
方法：直接將所有 特徵值相乘。即 $det(A) = \lambda_1 \times \lambda_2 \times \dots \times \lambda_n$。(3) 用LU分解快速計算
原理：利用高斯消去法將矩陣 $A$ 拆解為下三角矩陣 $L$ 與上三角矩陣 $U$。方法：由於三角矩陣的行列式等於對角線乘積，且通常設 $det(L)=1$，故 $det(A) = det(U) \text{ 的對角線乘積}$。

意義： 保證了我們可以用 [單一數值] 快速判斷系統的命運。若行列式為0，代表資訊流失、系統無解（奇異矩陣）；若不為0，代表系統資訊完整，且我們精確掌握了世界膨脹或縮小的 [倍率]。

----------------------------------------------------------
## 6.特徵值和特徵向量的意義是什麼？特徵值分解有何用途？
* 特徵值($\lambda$)與特徵向量($v$)是什麼？指在矩陣 ($A$) 的變換作用下，方向不改變、只改變長度的特殊向量 ($v$)，以及該向量被拉長或縮短的倍數 ($\lambda$)。
滿足公式：
$Av = \lambda v$。
* 特徵值分解有何用途？
它的核心用途是[解耦]與 [簡化運算]。它能找出系統原本的「主軸」，將複雜的矩陣互相糾纏的關係，拆解成獨立的單向運動，這讓計算矩陣的高次方。
* 特徵值分解代表的意義是什麼？
它代表 [提取關鍵特徵]。如果把數據看作一團雲霧，特徵向量就是指出這團雲霧「主要延伸的方向」（主成分），而特徵值告訴你它在那個方向「拉得有多長」。意義： 保證了我們能看透數據的 [本質結構]。它幫助我們忽略次要的雜訊，直接鎖定資訊量最大、變化最劇烈的維度。
----------------------------------------------------------
## 7.QR分解是什麼？
* 數學中的 [QR 分解] 是什麼？指將一個矩陣 $A$ 拆解為正交矩陣($Q$)與上三角矩陣 ($R$) 的乘積 ($A = QR$)。
* 為何需要 QR 分解？為了解決最小平方法。當方程式 $Ax=b$ 無解時(例如數據點太多，無法連成一條直線)，LU 分解會失效，而 QR 分解能幫我們算出誤差最小的「最佳解」。
* QR 分解代表的意義是什麼？它代表 [修正座標系]。如果 $A$ 是一組歪斜的座標軸，QR 分解就是把它「轉正」成互相垂直的標準軸 ($Q$)，並記錄旋轉的過程 ($R$)。意義： 保證了我們在面對無解的現實數據時，依然能找到 [最接近真理的答案]，且計算過程比其他方法更穩定（不易產生誤差）。
----------------------------------------------------------
## 8.如何反覆用 QR 分解，完成特徵值分解？
* 什麼是QR演算法？
一種利用QR分解來計算矩陣特徵值的數值迭代法。其步驟為:對矩陣$A$做QR分解 ($A_0 = Q_0 R_0$)，然後將 $Q$ 和 $R$ 順序顛倒相乘 得到新矩陣 ($A_1 = R_0 Q_0$)。如此反覆操作。
為何這樣能算出特徵值？
每次的顛倒相乘 ($RQ$) 就像是對矩陣進行一次[洗牌]與[擠壓]。隨著迭代次數增加，矩陣下方的元素會逐漸變成 0，慢慢收斂成一個 上三角矩陣。此時，對角線上的數值 就會收斂為矩陣的 特徵值。

意義： 保證了電腦能求解任意高階矩陣的特徵值。因為五次以上的多項式沒有公式解，我們無法手算特徵方程式，必須依賴這種迭代法讓答案[「]浮現]出來。
----------------------------------------------------------
## 9.SVD 分解是什麼？和特徵值分解有何關係？
* 數學中的 [SVD 分解] (奇異值分解)是什麼？
指將 任意形狀 的矩陣 ($m \times n$) 拆解成三個矩陣的乘積：$A = U \Sigma V^T$ 。其 $\Sigma$ 是對角矩陣，其對角線元素稱為奇異值。和特徵值分解有何關係？它是特徵值分解的 [通用升級版]。
(1) 適用範圍更廣：特徵值分解只能用於「方陣」，SVD 可用於「任何長方形矩陣」。(2) 數學連結：SVD 的奇異值其實就是 $A^T A$ 特徵值的 平方根。意義： 保證了 [每個矩陣都有完美歸宿]。即使矩陣不是方陣、不可逆或充滿雜訊，SVD 依然能將其拆解為最單純的「旋轉 $\to$ 伸縮 $\to$ 旋轉」三個步驟，是線性代數中最強大的分解工具。
----------------------------------------------------------
## 10.主成分分析是什麼？和 SVD 分解有何關係？
* 數據分析中的 [主成分分析] (PCA)是什麼？指一種降維技術。它將高維度、繁雜的數據投影到一個新的座標系中，保留數據差異最大（變異數最大）的方向，丟棄不重要的雜訊。
* 和SVD分解有何關係？SVD 是執行 PCA 的 [計算引擎]。只要對數據的「共變異數矩陣」或直接對「數據矩陣」做 SVD 分解，得到的 $V$ 矩陣(右奇異向量)指的就是 主成分方向，而奇異值的大小代表該方向保留了多少 資訊量。

意義:保證了我們可以用 [最少的數據代表最多的資訊]。它讓我們能從成千上萬的變數中，萃取出最重要的幾個關鍵因素是現代 AI 與圖像壓縮的基礎。